{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 것"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maximum Likelihood Estimation(MLE)\n",
    "최대 가능 우도법 // 데이터가 다양한 분포중에서 해당 분포로 나왔을 가능도 $\\\\$\n",
    "관측 데이터에 맞춰진 분포를 우도라고 함. 이렇게 찾아진 확률분포를 이용해서 Gradient Ascent(local maximum) & Grdient Descent(local minimum) 를 구함$\\\\$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e4af68dc10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.train, test 데이터를 활용한 softmax함수 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3]), torch.Size([8]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#m=8개의 samples데이터와 in_feature=3를 이용해서 0/1/2로 분류하는 softmax모델 만들기\n",
    "x_train = torch.FloatTensor([[1, 2, 1],\n",
    "                             [1, 3, 2],\n",
    "                             [1, 3, 4],\n",
    "                             [1, 5, 5],\n",
    "                             [1, 7, 5],\n",
    "                             [1, 2, 5],\n",
    "                             [1, 6, 6],\n",
    "                             [1, 7, 7]\n",
    "                            ])\n",
    "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "y_test = torch.LongTensor([1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #in_feature=3 -> 3개로 분류\n",
    "        self.linear = nn.Linear(3,3)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftmaxClassifier(\n",
       "  (linear): Linear(in_features=3, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SoftmaxClassifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train데이터에 대해서는 과적합되지 않는 선에서 여러번 epoch를 수행하므로써\n",
    "#model를 여러번학습시키므로써 적당히 분류기 모델을 찾는다\n",
    "def train(model, optimizer , x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        cost = F.cross_entropy(prediction,y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, optimizer, x_test, y_test):\n",
    "    prediction = model(x_test)\n",
    "\n",
    "    predicted_class = prediction.max(1)[1]\n",
    "    print( predicted_class)\n",
    "    correct_count = (predicted_class==y_test).sum()\n",
    "\n",
    "    cost = F.cross_entropy(prediction,y_test)\n",
    "\n",
    "    print('Accuracy: {}% Cost: {:.6f}'.format(correct_count / len(y_test) * 100, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 2.203667\n",
      "Epoch    1/20 Cost: 1.199645\n",
      "Epoch    2/20 Cost: 1.142985\n",
      "Epoch    3/20 Cost: 1.117769\n",
      "Epoch    4/20 Cost: 1.100901\n",
      "Epoch    5/20 Cost: 1.089523\n",
      "Epoch    6/20 Cost: 1.079872\n",
      "Epoch    7/20 Cost: 1.071320\n",
      "Epoch    8/20 Cost: 1.063325\n",
      "Epoch    9/20 Cost: 1.055720\n",
      "Epoch   10/20 Cost: 1.048378\n",
      "Epoch   11/20 Cost: 1.041245\n",
      "Epoch   12/20 Cost: 1.034285\n",
      "Epoch   13/20 Cost: 1.027478\n",
      "Epoch   14/20 Cost: 1.020813\n",
      "Epoch   15/20 Cost: 1.014279\n",
      "Epoch   16/20 Cost: 1.007872\n",
      "Epoch   17/20 Cost: 1.001586\n",
      "Epoch   18/20 Cost: 0.995419\n",
      "Epoch   19/20 Cost: 0.989365\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1])\n",
      "Accuracy: 33.333335876464844% Cost: 1.209368\n"
     ]
    }
   ],
   "source": [
    "test(model, optimizer, x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 학습률 lr를 조정\n",
    "learning rate가 너무 크면 diverge가 되면서 gradient=0으로 수렴하지 않고 오히려 발산해버린다\n",
    "learning rate가 너무 작으면 gradient=0으로 가는 속도가 너무 느리다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 1.280268\n",
      "Epoch    1/20 Cost: 976950.812500\n",
      "Epoch    2/20 Cost: 1279135.125000\n",
      "Epoch    3/20 Cost: 1198379.000000\n",
      "Epoch    4/20 Cost: 1098825.750000\n",
      "Epoch    5/20 Cost: 1968197.625000\n",
      "Epoch    6/20 Cost: 284763.250000\n",
      "Epoch    7/20 Cost: 1532260.125000\n",
      "Epoch    8/20 Cost: 1651504.000000\n",
      "Epoch    9/20 Cost: 521878.437500\n",
      "Epoch   10/20 Cost: 1397263.250000\n",
      "Epoch   11/20 Cost: 750986.250000\n",
      "Epoch   12/20 Cost: 918691.500000\n",
      "Epoch   13/20 Cost: 1487888.250000\n",
      "Epoch   14/20 Cost: 1582260.125000\n",
      "Epoch   15/20 Cost: 685818.062500\n",
      "Epoch   16/20 Cost: 1140048.750000\n",
      "Epoch   17/20 Cost: 940566.562500\n",
      "Epoch   18/20 Cost: 931638.250000\n",
      "Epoch   19/20 Cost: 1971322.625000\n"
     ]
    }
   ],
   "source": [
    "#learing rate가 너무 커섯 cost가 점점 커지는 현상(overshooting)\n",
    "train(model, optimizer, x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifier()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 3.187324\n",
      "Epoch    1/20 Cost: 3.187324\n",
      "Epoch    2/20 Cost: 3.187324\n",
      "Epoch    3/20 Cost: 3.187324\n",
      "Epoch    4/20 Cost: 3.187324\n",
      "Epoch    5/20 Cost: 3.187324\n",
      "Epoch    6/20 Cost: 3.187324\n",
      "Epoch    7/20 Cost: 3.187324\n",
      "Epoch    8/20 Cost: 3.187324\n",
      "Epoch    9/20 Cost: 3.187324\n",
      "Epoch   10/20 Cost: 3.187324\n",
      "Epoch   11/20 Cost: 3.187324\n",
      "Epoch   12/20 Cost: 3.187324\n",
      "Epoch   13/20 Cost: 3.187324\n",
      "Epoch   14/20 Cost: 3.187324\n",
      "Epoch   15/20 Cost: 3.187324\n",
      "Epoch   16/20 Cost: 3.187324\n",
      "Epoch   17/20 Cost: 3.187324\n",
      "Epoch   18/20 Cost: 3.187324\n",
      "Epoch   19/20 Cost: 3.187324\n"
     ]
    }
   ],
   "source": [
    "#learning rate가 너무 작아서 cost가 거의 줄어들지 않는 현상\n",
    "train(model, optimizer, x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifier()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 1.341574\n",
      "Epoch    1/20 Cost: 1.198802\n",
      "Epoch    2/20 Cost: 1.150877\n",
      "Epoch    3/20 Cost: 1.131978\n",
      "Epoch    4/20 Cost: 1.116242\n",
      "Epoch    5/20 Cost: 1.102514\n",
      "Epoch    6/20 Cost: 1.089676\n",
      "Epoch    7/20 Cost: 1.077479\n",
      "Epoch    8/20 Cost: 1.065775\n",
      "Epoch    9/20 Cost: 1.054511\n",
      "Epoch   10/20 Cost: 1.043655\n",
      "Epoch   11/20 Cost: 1.033187\n",
      "Epoch   12/20 Cost: 1.023091\n",
      "Epoch   13/20 Cost: 1.013356\n",
      "Epoch   14/20 Cost: 1.003968\n",
      "Epoch   15/20 Cost: 0.994917\n",
      "Epoch   16/20 Cost: 0.986189\n",
      "Epoch   17/20 Cost: 0.977775\n",
      "Epoch   18/20 Cost: 0.969661\n",
      "Epoch   19/20 Cost: 0.961836\n"
     ]
    }
   ],
   "source": [
    "#적당한 learning rate를 찾아야됨\n",
    "train(model, optimizer, x_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data preprocessing(데이터 전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3개의 feature를 활용해서 예측값을 추출하는 다중선형회귀\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([84.8000, 84.6000, 85.6000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = x_train.mean(dim=0)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11.0544, 12.2393, 12.6214])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = x_train.std(dim=0)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0674, -0.3758, -0.8398],\n",
       "        [ 0.7418,  0.2778,  0.5863],\n",
       "        [ 0.3799,  0.5229,  0.3486],\n",
       "        [ 1.0132,  1.0948,  1.1409],\n",
       "        [-1.0674, -1.5197, -1.2360]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = (x_train-m)/sigma\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateLinearRegressionModel(\n",
       "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        cost = F.mse_loss(prediction,y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 57621.960938\n",
      "Epoch    1/20 Cost: 1115690762240.000000\n",
      "Epoch    2/20 Cost: 21603378185380036608.000000\n",
      "Epoch    3/20 Cost: 418311281336289389130547200.000000\n",
      "Epoch    4/20 Cost: 8099859627174812776678955834933248.000000\n",
      "Epoch    5/20 Cost: inf\n",
      "Epoch    6/20 Cost: inf\n",
      "Epoch    7/20 Cost: inf\n",
      "Epoch    8/20 Cost: inf\n",
      "Epoch    9/20 Cost: inf\n",
      "Epoch   10/20 Cost: inf\n",
      "Epoch   11/20 Cost: inf\n",
      "Epoch   12/20 Cost: nan\n",
      "Epoch   13/20 Cost: nan\n",
      "Epoch   14/20 Cost: nan\n",
      "Epoch   15/20 Cost: nan\n",
      "Epoch   16/20 Cost: nan\n",
      "Epoch   17/20 Cost: nan\n",
      "Epoch   18/20 Cost: nan\n",
      "Epoch   19/20 Cost: nan\n"
     ]
    }
   ],
   "source": [
    "#데이터 스케일링이 너무커서 cost를 올바르게 계산을 못함\n",
    "train(model, optimizer, x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 29476.677734\n",
      "Epoch    1/20 Cost: 18728.175781\n",
      "Epoch    2/20 Cost: 11946.346680\n",
      "Epoch    3/20 Cost: 7634.267090\n",
      "Epoch    4/20 Cost: 4882.764648\n",
      "Epoch    5/20 Cost: 3124.194336\n",
      "Epoch    6/20 Cost: 1999.400635\n",
      "Epoch    7/20 Cost: 1279.729858\n",
      "Epoch    8/20 Cost: 819.193237\n",
      "Epoch    9/20 Cost: 524.461670\n",
      "Epoch   10/20 Cost: 335.832794\n",
      "Epoch   11/20 Cost: 215.106110\n",
      "Epoch   12/20 Cost: 137.836304\n",
      "Epoch   13/20 Cost: 88.378960\n",
      "Epoch   14/20 Cost: 56.721436\n",
      "Epoch   15/20 Cost: 36.456116\n",
      "Epoch   16/20 Cost: 23.481792\n",
      "Epoch   17/20 Cost: 15.174074\n",
      "Epoch   18/20 Cost: 9.852999\n",
      "Epoch   19/20 Cost: 6.443649\n"
     ]
    }
   ],
   "source": [
    "#스케일링한 데이터라서 잘 해결함\n",
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "train(model, optimizer, norm,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3 overfitting : epoch를 많이 시도하면 train data에 대해서는 성능이 좋게 나오지만, $\\\\$\n",
    "test data에서는 좋아지다가 일정시점이 된 후로는 다시 성능이 나빠짐.(성능의 기준은 cost를 기준으로)\n",
    "\n",
    "-overfitting대처법\n",
    "1. more data(많은 데이터를 활용)\n",
    "2. less features(feature의 개수를 줄임)\n",
    "3. Cross validation(교차 검정)\n",
    "4. Regularization(규제)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regularization\n",
    "특정 가중치가 너무 크면 과대적합이 발생할 수 있음 => 가중치를 조정한다=모델을 규제하는 방식 : L1(라쏘), L2(릿지), 엘라스틱넷$\\\\$\n",
    "\n",
    "-norm(벡터의 길이 or 크기를 측정하는 함수로 예측값-실제값의 거리르 재는 방식으로 사용된다) $\\\\$\n",
    "\n",
    "$.\\quad$ i) L1 Norm(Manhattan norm : 맨해튼 노름)\n",
    "$.\\quad$ 백터요소의 절댓값의 합 : L1 Norm = $\\sum|x_i|$ $\\\\$ 제곱까지는 하지 않아서 이상치에 영향을 덜 받음 => Lasso(L1 규제)회귀에서 loss값을 규제할 때 이용$\\\\$\n",
    "$.\\quad$ ii) L2 Norm(Euclidean norm : 유클리리단 노름) \n",
    "$.\\quad$ 피타고라스 정리를 이용한 최단거리 계산 : L2 Norm = $\\sqrt(\\sum x_i^2)$ $\\\\$제곱을 하므로 이상치에 영향을 받음 => Ridge(L2 규제)회귀에서 loss값을 규제할 때 이용$\\\\$\n",
    "\n",
    "1. Lasso규제 = L1 Regularization : j(W) = MSE(W) + $a\\sum |W_{i}|$ $\\\\$\n",
    "L1규제를 Lasso규제라고 부르는데, 기존 비용함수 MSE에서 |가중치|를 더해서 비용함수를 계산하는 것을 말함 $\\\\$\n",
    "Lasso규제의 목적은 중요하지 않은 가중치를 삭제하는 것이다. 즉, 가중치가 작은 W값들은 최적의 값을 찾아나가는 중에 점차 0이 되면서 아예 사라져버리게 된다$\\\\$\n",
    "이렇게 되면 feature의 개수가 줄어들게 되어서 overfitting를 방지할 수 있다. $\\\\$\n",
    "a : 모델을 얼마나 규제할 지 조정하는 하이퍼 파라미터이다. a가 크면 모든 가중치가 거의 0에 가깝게 설정이 가능하다.($\\sum |W_{i}|$의 중요도가 훨씬 커지므로) \n",
    "\n",
    "2. Ridge규제 = L2 Regularization : j(W) = MSE(W) + $\\frac{1}{2} a\\sum (W_{i})^2$ $\\\\$ \n",
    "$\\frac{1}{2}$ : 미분할 때 편하게 하기 위해 곱해줬다.$\\\\$\n",
    "L2규제를 Ridge규제라고 부르는데, Lasso부분의 |가중치| -> $(가중치)^2$로 바뀐것이다$\\\\$\n",
    "Ridge규제의 목적은 가중치가 너무 크지않은 방향으로 학습시키는 방식이다. $w^2$를 비용함수에 반영하다보니 너무 큰 W값들은 엄청 줄이게 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[-0.0270, -0.3854,  0.3516]], requires_grad=True) tensor(0.7640, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[-0.0270, -0.3854,  0.3516]], requires_grad=True) tensor(0.5224, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[-0.0270, -0.3854,  0.3516]], requires_grad=True) tensor(0.2729, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([0.1792], requires_grad=True) tensor(0.1792, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([0.1792], requires_grad=True) tensor(0.1792, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([0.1792], requires_grad=True) tensor(0.0321, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[3.5964, 3.1878, 3.9575]], requires_grad=True) tensor(10.7417, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[3.5964, 3.1878, 3.9575]], requires_grad=True) tensor(6.2256, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[3.5964, 3.1878, 3.9575]], requires_grad=True) tensor(38.7577, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([34.2433], requires_grad=True) tensor(34.2433, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([34.2433], requires_grad=True) tensor(34.2433, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([34.2433], requires_grad=True) tensor(1172.6068, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[5.5035, 5.0051, 5.8778]], requires_grad=True) tensor(16.3863, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[5.5035, 5.0051, 5.8778]], requires_grad=True) tensor(9.4809, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[5.5035, 5.0051, 5.8778]], requires_grad=True) tensor(89.8873, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([61.4947], requires_grad=True) tensor(61.4947, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([61.4947], requires_grad=True) tensor(61.4947, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([61.4947], requires_grad=True) tensor(3781.5955, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[6.5454, 5.9692, 6.9165]], requires_grad=True) tensor(19.4312, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[6.5454, 5.9692, 6.9165]], requires_grad=True) tensor(11.2389, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[6.5454, 5.9692, 6.9165]], requires_grad=True) tensor(126.3122, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([83.2957], requires_grad=True) tensor(83.2957, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([83.2957], requires_grad=True) tensor(83.2957, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([83.2957], requires_grad=True) tensor(6938.1812, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.1203, 6.4740, 7.4793]], requires_grad=True) tensor(21.0736, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.1203, 6.4740, 7.4793]], requires_grad=True) tensor(12.1881, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.1203, 6.4740, 7.4793]], requires_grad=True) tensor(148.5506, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([100.7366], requires_grad=True) tensor(100.7366, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([100.7366], requires_grad=True) tensor(100.7366, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([100.7366], requires_grad=True) tensor(10147.8613, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.4429, 6.7313, 7.7851]], requires_grad=True) tensor(21.9594, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.4429, 6.7313, 7.7851]], requires_grad=True) tensor(12.7010, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.4429, 6.7313, 7.7851]], requires_grad=True) tensor(161.3161, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([114.6893], requires_grad=True) tensor(114.6893, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([114.6893], requires_grad=True) tensor(114.6893, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([114.6893], requires_grad=True) tensor(13153.6309, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.6292, 6.8556, 7.9522]], requires_grad=True) tensor(22.4370, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.6292, 6.8556, 7.9522]], requires_grad=True) tensor(12.9785, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.6292, 6.8556, 7.9522]], requires_grad=True) tensor(168.4421, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([125.8514], requires_grad=True) tensor(125.8514, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([125.8514], requires_grad=True) tensor(125.8514, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([125.8514], requires_grad=True) tensor(15838.5811, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.7415, 6.9085, 8.0444]], requires_grad=True) tensor(22.6945, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.7415, 6.9085, 8.0444]], requires_grad=True) tensor(13.1291, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.7415, 6.9085, 8.0444]], requires_grad=True) tensor(172.3725, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([134.7811], requires_grad=True) tensor(134.7811, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([134.7811], requires_grad=True) tensor(134.7811, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([134.7811], requires_grad=True) tensor(18165.9570, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.8137, 6.9233, 8.0962]], requires_grad=True) tensor(22.8332, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.8137, 6.9233, 8.0962]], requires_grad=True) tensor(13.2112, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.8137, 6.9233, 8.0962]], requires_grad=True) tensor(174.5347, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([141.9249], requires_grad=True) tensor(141.9249, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([141.9249], requires_grad=True) tensor(141.9249, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([141.9249], requires_grad=True) tensor(20142.6797, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.8639, 6.9179, 8.1260]], requires_grad=True) tensor(22.9078, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.8639, 6.9179, 8.1260]], requires_grad=True) tensor(13.2563, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.8639, 6.9179, 8.1260]], requires_grad=True) tensor(175.7308, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([147.6399], requires_grad=True) tensor(147.6399, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([147.6399], requires_grad=True) tensor(147.6399, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([147.6399], requires_grad=True) tensor(21797.5469, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.9020, 6.9019, 8.1440]], requires_grad=True) tensor(22.9479, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.9020, 6.9019, 8.1440]], requires_grad=True) tensor(13.2817, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.9020, 6.9019, 8.1440]], requires_grad=True) tensor(176.4025, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([152.2119], requires_grad=True) tensor(152.2119, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([152.2119], requires_grad=True) tensor(152.2119, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([152.2119], requires_grad=True) tensor(23168.4766, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.9332, 6.8806, 8.1555]], requires_grad=True) tensor(22.9693, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.9332, 6.8806, 8.1555]], requires_grad=True) tensor(13.2963, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.9332, 6.8806, 8.1555]], requires_grad=True) tensor(176.7908, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([155.8696], requires_grad=True) tensor(155.8696, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([155.8696], requires_grad=True) tensor(155.8696, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([155.8696], requires_grad=True) tensor(24295.3184, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.9605, 6.8568, 8.1634]], requires_grad=True) tensor(22.9807, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.9605, 6.8568, 8.1634]], requires_grad=True) tensor(13.3051, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.9605, 6.8568, 8.1634]], requires_grad=True) tensor(177.0262, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([158.7956], requires_grad=True) tensor(158.7956, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([158.7956], requires_grad=True) tensor(158.7956, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([158.7956], requires_grad=True) tensor(25216.0547, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[7.9853, 6.8319, 8.1694]], requires_grad=True) tensor(22.9867, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[7.9853, 6.8319, 8.1694]], requires_grad=True) tensor(13.3109, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[7.9853, 6.8319, 8.1694]], requires_grad=True) tensor(177.1797, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([161.1365], requires_grad=True) tensor(161.1365, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([161.1365], requires_grad=True) tensor(161.1365, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([161.1365], requires_grad=True) tensor(25964.9727, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[8.0086, 6.8068, 8.1742]], requires_grad=True) tensor(22.9897, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[8.0086, 6.8068, 8.1742]], requires_grad=True) tensor(13.3150, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[8.0086, 6.8068, 8.1742]], requires_grad=True) tensor(177.2892, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([163.0092], requires_grad=True) tensor(163.0092, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([163.0092], requires_grad=True) tensor(163.0092, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([163.0092], requires_grad=True) tensor(26572., grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[8.0308, 6.7819, 8.1784]], requires_grad=True) tensor(22.9912, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[8.0308, 6.7819, 8.1784]], requires_grad=True) tensor(13.3182, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[8.0308, 6.7819, 8.1784]], requires_grad=True) tensor(177.3752, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([164.5074], requires_grad=True) tensor(164.5074, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([164.5074], requires_grad=True) tensor(164.5074, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([164.5074], requires_grad=True) tensor(27062.6699, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[8.0522, 6.7574, 8.1822]], requires_grad=True) tensor(22.9918, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[8.0522, 6.7574, 8.1822]], requires_grad=True) tensor(13.3210, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[8.0522, 6.7574, 8.1822]], requires_grad=True) tensor(177.4488, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([165.7059], requires_grad=True) tensor(165.7059, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([165.7059], requires_grad=True) tensor(165.7059, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([165.7059], requires_grad=True) tensor(27458.4414, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[8.0729, 6.7335, 8.1857]], requires_grad=True) tensor(22.9920, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[8.0729, 6.7335, 8.1857]], requires_grad=True) tensor(13.3235, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[8.0729, 6.7335, 8.1857]], requires_grad=True) tensor(177.5159, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([166.6647], requires_grad=True) tensor(166.6647, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([166.6647], requires_grad=True) tensor(166.6647, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([166.6647], requires_grad=True) tensor(27777.1230, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[8.0929, 6.7100, 8.1890]], requires_grad=True) tensor(22.9920, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[8.0929, 6.7100, 8.1890]], requires_grad=True) tensor(13.3259, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[8.0929, 6.7100, 8.1890]], requires_grad=True) tensor(177.5796, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([167.4318], requires_grad=True) tensor(167.4318, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([167.4318], requires_grad=True) tensor(167.4318, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([167.4318], requires_grad=True) tensor(28033.3945, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([[8.1124, 6.6872, 8.1921]], requires_grad=True) tensor(22.9918, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([[8.1124, 6.6872, 8.1921]], requires_grad=True) tensor(13.3282, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([[8.1124, 6.6872, 8.1921]], requires_grad=True) tensor(177.6415, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "L1 norm(L1 Regularization) :  Parameter containing:\n",
      "tensor([168.0454], requires_grad=True) tensor(168.0454, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 norm :  Parameter containing:\n",
      "tensor([168.0454], requires_grad=True) tensor(168.0454, grad_fn=<NormBackward1>)\n",
      "\n",
      "L2 Regularization :  Parameter containing:\n",
      "tensor([168.0454], requires_grad=True) tensor(28239.2598, grad_fn=<MulBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#L1 norm, L2 norm, Lasso규제, Ridge규제를 직접 작성한 코드\n",
    "def train_with_regularization(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # l2 norm 계산\n",
    "        l2_reg = 0\n",
    "        \n",
    "        #param은 각 feature의 가중치를 의미함\n",
    "        #torch.norm(여러값들) : 텐서의 모든 값들을 제곱해서 더한후 제곱근(루트)를 씌운 하나의 scalar값\n",
    "        for param in model.parameters():\n",
    "\n",
    "            #L1 norm = L1 Regularization 사용 => |W|의 합\n",
    "            print(\"L1 norm(L1 Regularization) : \",param,torch.norm(param,p=1),end='\\n\\n')\n",
    "\n",
    "            #L2 norm => 피타고라스 최단거리\n",
    "            print(\"L2 norm : \",param,torch.norm(param,p=2),end='\\n\\n')\n",
    "\n",
    "            #L2 Regularization 사용 => |W|제곱의 합 = 위에서 구한 L2 norm의 제곱\n",
    "            print(\"L2 Regularization : \",param,torch.norm(param,p=2)*torch.norm(param,p=2),end=\"\\n\\n----------------------------------------------------------------------------\\n\")\n",
    "\n",
    "            #Regulaization을 사용할 때, L2 Norm를 사용하는 것도 이상한데, \n",
    "            #L2규제는 기존 MSE+가중치제곱의합을 더해주는건데,\n",
    "            #여기서는 MSE+가중치제곱의합의 루트를 더해준다.\n",
    "            #Regularization을 사용할건데, 왜 L2 norm를 갑자기 사용하냐?\n",
    "            #https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-07_1_tips.ipynb\n",
    "            l2_reg += torch.norm(param)\n",
    "        cost += l2_reg\n",
    "\n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch+1, nb_epochs, cost.item() ))\n",
    "\n",
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "train_with_regularization(model, optimizer, norm, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95f1ed8339fc15ac7f89ecef91cfc88a009aaf2bb1573f871691d038972c2dcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
