{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d527689bf0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature가 3개인 방식은 단순선형회귀로는 불가능\n",
    "#독립변수가 여러개일 때는, 다중선형회귀 사용\n",
    "#총 5명의 사람의 성적데이터(feature3개) => 각 사람의 기말고사 성적을 예측함\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 적접 hypothesis = x1*w1 + x2*w2 + x3*w3 + .......xn*wn + b형태로 작성하면, 독립변수가 많을 때 힘듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.297 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.676 w3: 0.676 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.677 w3: 0.677 b: 0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.677 w3: 0.677 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.678 w3: 0.678 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.678 w3: 0.678 b: 0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.679 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.679 w3: 0.679 b: 0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.679 w3: 0.679 b: 0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([w1,w2,w3,b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis = x1_train*w1 + x2_train*w2 + x3_train*w3 + b\n",
    "\n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, w1.item(), w3.item(), w3.item(), b.item(), cost.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 직접 hypothessis를 작성하지 않고 행렬곱을 이용해서 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  80.,  75.],\n",
       "        [ 93.,  88.,  93.],\n",
       "        [ 89.,  91.,  90.],\n",
       "        [ 96.,  98., 100.],\n",
       "        [ 73.,  66.,  70.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.hstack([x1_train,x2_train,x3_train])\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]], requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((3,1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((3,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W,b], lr=1e-5)\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    #x_train x W를 행렬곱으로 표현하면 길게 작성할 필요 없음\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, w1.item(), w3.item(), w3.item(), b.item(), cost.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module를 이용해서 다중선형회귀 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self , x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 31667.599609\n",
      "Epoch    1/20 Cost: 9926.266602\n",
      "Epoch    2/20 Cost: 3111.513916\n",
      "Epoch    3/20 Cost: 975.451355\n",
      "Epoch    4/20 Cost: 305.908539\n",
      "Epoch    5/20 Cost: 96.042496\n",
      "Epoch    6/20 Cost: 30.260748\n",
      "Epoch    7/20 Cost: 9.641701\n",
      "Epoch    8/20 Cost: 3.178671\n",
      "Epoch    9/20 Cost: 1.152871\n",
      "Epoch   10/20 Cost: 0.517863\n",
      "Epoch   11/20 Cost: 0.318801\n",
      "Epoch   12/20 Cost: 0.256388\n",
      "Epoch   13/20 Cost: 0.236821\n",
      "Epoch   14/20 Cost: 0.230660\n",
      "Epoch   15/20 Cost: 0.228719\n",
      "Epoch   16/20 Cost: 0.228095\n",
      "Epoch   17/20 Cost: 0.227880\n",
      "Epoch   18/20 Cost: 0.227799\n",
      "Epoch   19/20 Cost: 0.227762\n"
     ]
    }
   ],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs=20\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    #5명의 사람의 기말고사 성적예측데이터\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    cost = F.mse_loss(prediction,y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data => 왜 DataSet과 DataLoader를 사용하는지 모르겠음 <= 데이터를 나누는 작업을 하는 이유는 알음(속도, 과적합등) 그런데 아래 batch만큼 나눴는데, 어디서 티가 나는거지??????"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minibatch Gradient Descent\n",
    "##### 전체 데이터를 균일하게 나눠서 학습하자(가금 잘못된 방향으로 업데이트 할 수도 있음)\n",
    "1. 사용자 정의 데이터 셋 만들기 => 하나의 샘플(객체)에 feature-label를 지정함\n",
    "2. minibatch를 이용해서 데이터를 섞음 => 미니배치로 전달, epoch마다 다시 섞어서 과적합을 방지함\n",
    "3. 해당 객체를 이용해서 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([73., 80., 75.]), tensor([152.])),\n",
       " (tensor([93., 88., 93.]), tensor([185.])),\n",
       " (tensor([89., 91., 90.]), tensor([180.])),\n",
       " (tensor([ 96.,  98., 100.]), tensor([196.])),\n",
       " (tensor([73., 66., 70.]), tensor([142.]))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch dataset만들기\n",
    "#직접 상속받아 커스텀 데이터셋을 만듬\n",
    "#Dataset클래스는 3개 함수를 구현해야함 => __init__ , __len__ , __getitem__\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#Dataset상속\n",
    "class CustomDataSet(Dataset):\n",
    "\n",
    "    #객체를 생성할 때 실행되는 함수로 feature과 label를 설정함\n",
    "    def __init__(self):\n",
    "        self.x_data = x_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    #데이터 샘플 개수 반환\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    #해당 인덱스idx에 대응하는 데이터 반환\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "       \n",
    "        return x,y\n",
    "\n",
    "dataset = CustomDataSet()\n",
    "list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[93., 88., 93.]]), tensor([[185.]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#minibactch의 크기(2의 제곱수로 설정)\n",
    "#shuffle 각 epoch마다 데이터셋을 다시 섞음(모델이 순서를 외우지 않도록) => 전체데이터개수=5 / batch_size=2 = 2.5이면 총 3번의 iteration을 수행하게 됨\n",
    "#이렇게 작은 단위로 데이터셋을 나눠서 분석하는 방식을 minibatch라고 함\n",
    "dataloader = DataLoader( dataset , batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[73., 66., 70.],\n",
       "          [73., 80., 75.]]),\n",
       "  tensor([[142.],\n",
       "          [152.]])],\n",
       " [tensor([[ 89.,  91.,  90.],\n",
       "          [ 96.,  98., 100.]]),\n",
       "  tensor([[180.],\n",
       "          [196.]])],\n",
       " [tensor([[93., 88., 93.]]), tensor([[185.]])]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch_size를 2로 설정하면 5/2=2.5=3개로 batch를 나눔\n",
    "#batch_size를 3로 설정하면 5/3=1.6=2개로 batch를 나눔\n",
    "list(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 0.444082\n",
      "Epoch    0/20 Batch 2/3 Cost: 0.184806\n",
      "Epoch    0/20 Batch 3/3 Cost: 0.035042\n",
      "Epoch    1/20 Batch 1/3 Cost: 0.131975\n",
      "Epoch    1/20 Batch 2/3 Cost: 0.508313\n",
      "Epoch    1/20 Batch 3/3 Cost: 0.005707\n",
      "Epoch    2/20 Batch 1/3 Cost: 0.415352\n",
      "Epoch    2/20 Batch 2/3 Cost: 0.020756\n",
      "Epoch    2/20 Batch 3/3 Cost: 0.337660\n",
      "Epoch    3/20 Batch 1/3 Cost: 0.548951\n",
      "Epoch    3/20 Batch 2/3 Cost: 0.196130\n",
      "Epoch    3/20 Batch 3/3 Cost: 0.086799\n",
      "Epoch    4/20 Batch 1/3 Cost: 0.103543\n",
      "Epoch    4/20 Batch 2/3 Cost: 0.529867\n",
      "Epoch    4/20 Batch 3/3 Cost: 0.008752\n",
      "Epoch    5/20 Batch 1/3 Cost: 0.220369\n",
      "Epoch    5/20 Batch 2/3 Cost: 0.040563\n",
      "Epoch    5/20 Batch 3/3 Cost: 0.932337\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.218245\n",
      "Epoch    6/20 Batch 2/3 Cost: 0.640859\n",
      "Epoch    6/20 Batch 3/3 Cost: 0.006037\n",
      "Epoch    7/20 Batch 1/3 Cost: 0.461589\n",
      "Epoch    7/20 Batch 2/3 Cost: 0.011903\n",
      "Epoch    7/20 Batch 3/3 Cost: 0.294716\n",
      "Epoch    8/20 Batch 1/3 Cost: 0.567194\n",
      "Epoch    8/20 Batch 2/3 Cost: 0.010910\n",
      "Epoch    8/20 Batch 3/3 Cost: 0.240944\n",
      "Epoch    9/20 Batch 1/3 Cost: 0.620051\n",
      "Epoch    9/20 Batch 2/3 Cost: 0.025722\n",
      "Epoch    9/20 Batch 3/3 Cost: 0.063858\n",
      "Epoch   10/20 Batch 1/3 Cost: 0.114807\n",
      "Epoch   10/20 Batch 2/3 Cost: 0.044036\n",
      "Epoch   10/20 Batch 3/3 Cost: 0.975102\n",
      "Epoch   11/20 Batch 1/3 Cost: 0.237886\n",
      "Epoch   11/20 Batch 2/3 Cost: 0.153029\n",
      "Epoch   11/20 Batch 3/3 Cost: 0.874477\n",
      "Epoch   12/20 Batch 1/3 Cost: 0.403252\n",
      "Epoch   12/20 Batch 2/3 Cost: 0.305843\n",
      "Epoch   12/20 Batch 3/3 Cost: 0.016562\n",
      "Epoch   13/20 Batch 1/3 Cost: 0.050716\n",
      "Epoch   13/20 Batch 2/3 Cost: 0.489800\n",
      "Epoch   13/20 Batch 3/3 Cost: 0.410596\n",
      "Epoch   14/20 Batch 1/3 Cost: 0.519176\n",
      "Epoch   14/20 Batch 2/3 Cost: 0.068572\n",
      "Epoch   14/20 Batch 3/3 Cost: 0.212506\n",
      "Epoch   15/20 Batch 1/3 Cost: 0.067711\n",
      "Epoch   15/20 Batch 2/3 Cost: 0.051683\n",
      "Epoch   15/20 Batch 3/3 Cost: 1.180023\n",
      "Epoch   16/20 Batch 1/3 Cost: 0.036114\n",
      "Epoch   16/20 Batch 2/3 Cost: 0.490457\n",
      "Epoch   16/20 Batch 3/3 Cost: 0.211143\n",
      "Epoch   17/20 Batch 1/3 Cost: 0.023206\n",
      "Epoch   17/20 Batch 2/3 Cost: 0.127073\n",
      "Epoch   17/20 Batch 3/3 Cost: 1.149263\n",
      "Epoch   18/20 Batch 1/3 Cost: 0.493727\n",
      "Epoch   18/20 Batch 2/3 Cost: 0.158420\n",
      "Epoch   18/20 Batch 3/3 Cost: 0.010627\n",
      "Epoch   19/20 Batch 1/3 Cost: 0.414848\n",
      "Epoch   19/20 Batch 2/3 Cost: 0.193378\n",
      "Epoch   19/20 Batch 3/3 Cost: 0.005080\n",
      "Epoch   20/20 Batch 1/3 Cost: 0.429329\n",
      "Epoch   20/20 Batch 2/3 Cost: 0.291166\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.064298\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    for batch_idx , samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "      \n",
    "        prediction = model(x_train)\n",
    "\n",
    "        cost = F.mse_loss(prediction,y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}\".format(epoch,nb_epochs,batch_idx+1, len(dataloader), cost.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[73., 66., 70.],\n",
       "          [73., 80., 75.]]),\n",
       "  tensor([[142.],\n",
       "          [152.]])],\n",
       " [tensor([[ 89.,  91.,  90.],\n",
       "          [ 96.,  98., 100.]]),\n",
       "  tensor([[180.],\n",
       "          [196.]])],\n",
       " [tensor([[93., 88., 93.]]), tensor([[185.]])]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95f1ed8339fc15ac7f89ecef91cfc88a009aaf2bb1573f871691d038972c2dcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
